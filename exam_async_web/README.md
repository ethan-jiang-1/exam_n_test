# AsyncWebFetcher - å¼‚æ­¥ç½‘é¡µå†…å®¹è·å–å™¨

è¿™æ˜¯ä¸€ä¸ªåŸºäº Python `aiohttp` çš„å¼‚æ­¥ç½‘é¡µå†…å®¹è·å–å™¨ï¼Œæ”¯æŒå¹¶å‘è·å–ã€è¶…æ—¶æ§åˆ¶ã€è‡ªå®šä¹‰è¯·æ±‚å¤´å’ŒäºŒè¿›åˆ¶å†…å®¹ä¸‹è½½ã€‚

## åŠŸèƒ½ç‰¹æ€§

- âœ¨ å¼‚æ­¥å¹¶å‘è·å–
- ğŸ”„ å¯æ§åˆ¶çš„å¹¶å‘æ•°é‡
- â±ï¸ è¶…æ—¶æ§åˆ¶
- ğŸ“ è‡ªå®šä¹‰è¯·æ±‚å¤´
- ğŸ–¼ï¸ äºŒè¿›åˆ¶å†…å®¹æ”¯æŒï¼ˆå¦‚å›¾ç‰‡ä¸‹è½½ï¼‰
- ğŸš¦ å®Œå–„çš„é”™è¯¯å¤„ç†
- ğŸ“Š è¯¦ç»†çš„è·å–çŠ¶æ€

## å®‰è£…ä¾èµ–

```bash
pip install aiohttp
```

## ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ç”¨æ³•

```python
import asyncio
from async_web_fetcher import download_urls

async def main():
    urls = [
        "https://example.com",
        "https://example.org"
    ]
    results = await download_urls(urls)
    for result in results:
        print(f"URL: {result['url']}, Status: {result['status']}")

if __name__ == "__main__":
    asyncio.run(main())
```

### é«˜çº§ç”¨æ³•

```python
async def main():
    # è‡ªå®šä¹‰é…ç½®
    urls = ["https://example.com/image.png"]
    results = await download_urls(
        urls,
        max_concurrent=3,        # æœ€å¤§å¹¶å‘æ•°
        timeout=30,             # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        headers={               # è‡ªå®šä¹‰è¯·æ±‚å¤´
            "User-Agent": "CustomBot/1.0",
            "Accept": "image/png"
        },
        as_binary=True         # ä¸‹è½½äºŒè¿›åˆ¶å†…å®¹
    )
```

## API æ–‡æ¡£

### AsyncWebFetcher ç±»

ä¸»è¦çš„å†…å®¹è·å–å™¨ç±»ï¼Œæ”¯æŒå¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚

#### åˆå§‹åŒ–å‚æ•°

- `max_concurrent` (int, é»˜è®¤=5): æœ€å¤§å¹¶å‘è·å–æ•°
- `timeout` (int, é»˜è®¤=30): è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

#### æ–¹æ³•

##### async fetch_page(url: str, headers: Optional[Dict] = None, as_binary: bool = False)

è·å–å•ä¸ªé¡µé¢å†…å®¹ã€‚

å‚æ•°:
- `url`: ç›®æ ‡URL
- `headers`: å¯é€‰çš„è¯·æ±‚å¤´å­—å…¸
- `as_binary`: æ˜¯å¦ä»¥äºŒè¿›åˆ¶æ¨¡å¼è·å–

è¿”å›:
```python
{
    "url": str,          # è¯·æ±‚çš„URL
    "status": str,       # çŠ¶æ€ç æˆ–çŠ¶æ€ï¼ˆ"200"/"error"/"timeout"ï¼‰
    "content": Union[str, bytes],  # å“åº”å†…å®¹
    "headers": Dict      # å“åº”å¤´
}
```

##### async fetch_pages(urls: List[str], headers: Optional[Dict] = None, as_binary: bool = False)

å¹¶å‘è·å–å¤šä¸ªé¡µé¢å†…å®¹ã€‚

å‚æ•°:
- `urls`: URLåˆ—è¡¨
- `headers`: å¯é€‰çš„è¯·æ±‚å¤´å­—å…¸
- `as_binary`: æ˜¯å¦ä»¥äºŒè¿›åˆ¶æ¨¡å¼è·å–

è¿”å›:
- è¿”å›ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ ¼å¼åŒ `fetch_page`

### ä¾¿æ·å‡½æ•°

#### async download_urls(...)

ä¾¿æ·çš„è·å–å‡½æ•°ï¼Œè‡ªåŠ¨å¤„ç†è·å–å™¨çš„åˆ›å»ºå’Œæ¸…ç†ã€‚

å‚æ•°:
- `urls`: URLåˆ—è¡¨
- `max_concurrent`: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤=5ï¼‰
- `timeout`: è¶…æ—¶æ—¶é—´ï¼ˆé»˜è®¤=30ç§’ï¼‰
- `headers`: å¯é€‰çš„è¯·æ±‚å¤´å­—å…¸
- `as_binary`: æ˜¯å¦ä»¥äºŒè¿›åˆ¶æ¨¡å¼è·å–

## é”™è¯¯å¤„ç†

è·å–å™¨ä¼šå¤„ç†ä»¥ä¸‹é”™è¯¯æƒ…å†µï¼š
- ç½‘ç»œè¿æ¥é”™è¯¯
- DNSè§£æé”™è¯¯
- è¶…æ—¶é”™è¯¯
- HTTPé”™è¯¯ï¼ˆå¦‚404ï¼‰

æ‰€æœ‰é”™è¯¯éƒ½ä¼šè¢«æ•è·å¹¶è¿”å›é€‚å½“çš„çŠ¶æ€ï¼Œä¸ä¼šæŠ›å‡ºå¼‚å¸¸ã€‚

## ç¤ºä¾‹

æŸ¥çœ‹ `fetcher_demo.py` è·å–å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ï¼š
1. æ™®é€šç½‘é¡µè·å–
2. è‡ªå®šä¹‰è¯·æ±‚å¤´
3. äºŒè¿›åˆ¶å†…å®¹è·å–
4. é”™è¯¯å¤„ç†
5. è¶…æ—¶å¤„ç†

## æ³¨æ„äº‹é¡¹

1. ä½¿ç”¨ `AsyncWebFetcher` æ—¶å¿…é¡»ä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆasync withï¼‰
2. è®¾ç½®åˆé€‚çš„å¹¶å‘æ•°å’Œè¶…æ—¶æ—¶é—´ä»¥é¿å…æœåŠ¡å™¨è¿‡è½½
3. è·å–å¤§æ–‡ä»¶æ—¶å»ºè®®ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼ï¼ˆas_binary=Trueï¼‰ 